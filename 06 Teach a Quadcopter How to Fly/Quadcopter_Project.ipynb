{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! \n",
    "\n",
    "Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Take a look at the files in the directory to better understand the structure of the project. \n",
    "\n",
    "- `task.py`: Define your task (environment) in this file.\n",
    "- `agents/`: Folder containing reinforcement learning agents.\n",
    "    - `policy_search.py`: A sample agent has been provided here.\n",
    "    - `agent.py`: Develop your agent here.\n",
    "- `physics_sim.py`: This file contains the simulator for the quadcopter.  **DO NOT MODIFY THIS FILE**.\n",
    "\n",
    "For this project, you will define your own task in `task.py`.  Although we have provided a example task to get you started, you are encouraged to change it.  Later in this notebook, you will learn more about how to amend this file.\n",
    "\n",
    "You will also design a reinforcement learning agent in `agent.py` to complete your chosen task.  \n",
    "\n",
    "You are welcome to create any additional files to help you to organize your code.  For instance, you may find it useful to define a `model.py` file defining any needed neural network architectures.\n",
    "\n",
    "## Controlling the Quadcopter\n",
    "\n",
    "We provide a sample agent in the code cell below to show you how to use the sim to control the quadcopter.  This agent is even simpler than the sample agent that you'll examine (in `agents/policy_search.py`) later in this notebook!\n",
    "\n",
    "The agent controls the quadcopter by setting the revolutions per second on each of its four rotors.  The provided agent in the `Basic_Agent` class below always selects a random action for each of the four rotors.  These four speeds are returned by the `act` method as a list of four floating-point numbers.  \n",
    "\n",
    "For this project, the agent that you will implement in `agents/agent.py` will have a far more intelligent method for selecting actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Basic_Agent():\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "    \n",
    "    def act(self):\n",
    "        new_thrust = random.gauss(450., 25.)\n",
    "        return [new_thrust + random.gauss(0., 1.) for x in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to have the agent select actions to control the quadcopter.  \n",
    "\n",
    "Feel free to change the provided values of `runtime`, `init_pose`, `init_velocities`, and `init_angle_velocities` below to change the starting conditions of the quadcopter.\n",
    "\n",
    "The `labels` list below annotates statistics that are saved while running the simulation.  All of this information is saved in a text file `data.txt` and stored in the dictionary `results`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from tasks import ExampleTask\n",
    "\n",
    "# Modify the values below to give the quadcopter a different starting position.\n",
    "runtime = 5.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 10., 0., 0., 0.])  # initial pose\n",
    "init_velocities = np.array([0., 0., 0.])         # initial velocities\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\n",
    "file_output = 'data.txt'                         # file name for saved results\n",
    "\n",
    "# Setup\n",
    "task = ExampleTask(init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "agent = Basic_Agent(task)\n",
    "done = False\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "# Run the simulation, and save the results.\n",
    "with open(file_output, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)\n",
    "    while True:\n",
    "        rotor_speeds = agent.act()\n",
    "        _, _, done = task.step(rotor_speeds)\n",
    "        to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n",
    "        for ii in range(len(labels)):\n",
    "            results[labels[ii]].append(to_write[ii])\n",
    "        writer.writerow(to_write)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to visualize how the position of the quadcopter evolved during the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results['time'], results['x'], label='x')\n",
    "plt.plot(results['time'], results['y'], label='y')\n",
    "plt.plot(results['time'], results['z'], label='z')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell visualizes the velocity of the quadcopter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['x_velocity'], label='x_hat')\n",
    "plt.plot(results['time'], results['y_velocity'], label='y_hat')\n",
    "plt.plot(results['time'], results['z_velocity'], label='z_hat')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can plot the Euler angles (the rotation of the quadcopter over the $x$-, $y$-, and $z$-axes),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['phi'], label='phi')\n",
    "plt.plot(results['time'], results['theta'], label='theta')\n",
    "plt.plot(results['time'], results['psi'], label='psi')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before plotting the velocities (in radians per second) corresponding to each of the Euler angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['phi_velocity'], label='phi_velocity')\n",
    "plt.plot(results['time'], results['theta_velocity'], label='theta_velocity')\n",
    "plt.plot(results['time'], results['psi_velocity'], label='psi_velocity')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the code cell below to print the agent's choice of actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['rotor_speed1'], label='Rotor 1 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed2'], label='Rotor 2 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed3'], label='Rotor 3 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed4'], label='Rotor 4 revolutions / second')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying a task, you will derive the environment state from the simulator.  Run the code cell below to print the values of the following variables at the end of the simulation:\n",
    "- `task.sim.pose` (the position of the quadcopter in ($x,y,z$) dimensions and the Euler angles),\n",
    "- `task.sim.v` (the velocity of the quadcopter in ($x,y,z$) dimensions), and\n",
    "- `task.sim.angular_v` (radians/second for each of the three Euler angles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the pose, velocity, and angular velocity of the quadcopter at the end of the episode\n",
    "print(task.sim.pose)\n",
    "print(task.sim.v)\n",
    "print(task.sim.angular_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample task in `task.py`, we use the 6-dimensional pose of the quadcopter to construct the state of the environment at each timestep.  However, when amending the task for your purposes, you are welcome to expand the size of the state vector by including the velocity information.  You can use any combination of the pose, velocity, and angular velocity - feel free to tinker here, and construct the state to suit your task.\n",
    "\n",
    "## The Task\n",
    "\n",
    "A sample task has been provided for you in `task.py`.  Open this file in a new window now. \n",
    "\n",
    "The `__init__()` method is used to initialize several variables that are needed to specify the task.  \n",
    "- The simulator is initialized as an instance of the `PhysicsSim` class (from `physics_sim.py`).  \n",
    "- Inspired by the methodology in the original DDPG paper, we make use of action repeats.  For each timestep of the agent, we step the simulation `action_repeats` timesteps.  If you are not familiar with action repeats, please read the **Results** section in [the DDPG paper](https://arxiv.org/abs/1509.02971).\n",
    "- We set the number of elements in the state vector.  For the sample task, we only work with the 6-dimensional pose information.  To set the size of the state (`state_size`), we must take action repeats into account.  \n",
    "- The environment will always have a 4-dimensional action space, with one entry for each rotor (`action_size=4`). You can set the minimum (`action_low`) and maximum (`action_high`) values of each entry here.\n",
    "- The sample task in this provided file is for the agent to reach a target position.  We specify that target position as a variable.\n",
    "\n",
    "The `reset()` method resets the simulator.  The agent should call this method every time the episode ends.  You can see an example of this in the code cell below.\n",
    "\n",
    "The `step()` method is perhaps the most important.  It accepts the agent's choice of action `rotor_speeds`, which is used to prepare the next state to pass on to the agent.  Then, the reward is computed from `get_reward()`.  The episode is considered done if the time limit has been exceeded, or the quadcopter has travelled outside of the bounds of the simulation.\n",
    "\n",
    "In the next section, you will learn how to test the performance of an agent on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The sample agent given in `agents/policy_search.py` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (`score`), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise.\n",
    "\n",
    "Run the code cell below to see how the agent performs on the sample task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from agents import PolicySearchAgent\n",
    "from tasks import ExampleTask\n",
    "\n",
    "num_episodes = 1000\n",
    "target_pos = np.array([0., 0., 10.])\n",
    "task = ExampleTask(target_pos=target_pos)\n",
    "agent = PolicySearchAgent(task) \n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode() # start a new episode\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "            break\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent should perform very poorly on this task.  And that's where you come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the Task, Design the Agent, and Train Your Agent!\n",
    "\n",
    "Amend `task.py` to specify a task of your choosing.  If you're unsure what kind of task to specify, you may like to teach your quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n",
    "\n",
    "After specifying your task, use the sample agent in `agents/policy_search.py` as a template to define your own agent in `agents/agent.py`.  You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n",
    "\n",
    "Note that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning.\n",
    "\n",
    "---\n",
    "\n",
    "I want to implement TD3. Pseudocode is:\n",
    "\n",
    "![TD3 Algorithm](td3-pseudocode.png)\n",
    "\n",
    "Reference Implementation is on [GitHub](https://github.com/sfujim/TD3).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TODO: Train your agent here.\n",
    "\n",
    "# needed imports -> I decided to use PyTorch to learn something not seen in the nanodegree until now\n",
    "import torch.nn.init as init\n",
    "from agents import TD3Agent\n",
    "from tasks import Task\n",
    "from utils import ReplayBuffer\n",
    "from collections import deque\n",
    "\n",
    "# set global configuration DICT\n",
    "config = {\n",
    "    'INIT_FN_ACTOR': init.uniform_,       # function to use for weight initialization, see torch.nn.init\n",
    "    'INIT_W_MAX_ACTOR': 0.003,            # maximum value to use if uniform initialization of actor is used\n",
    "    'INIT_W_MIN_ACTOR': -0.003,           # minimum value to use if uniform initialization of actor is used\n",
    "    'INIT_W_ACTOR': 0.,                   # fixed value to use if init.constant_ initialization is used\n",
    "    'INIT_FN_CRITIC': init.uniform_,      # function to use for weight initialization, see torch.nn.init\n",
    "    'INIT_W_MAX_CRITIC': 0.003,           # maximum value to use if uniform initialization of critic is used\n",
    "    'INIT_W_MIN_CRITIC': -0.003,          # minimum value to use if uniform initialization of critic is used\n",
    "    'INIT_W_CRITIC': 0.,                  # fixed value to use if init.constant_ initialization is used\n",
    "    'LR_ACTOR': 0.0001,                   # learning rate of actor optimizer\n",
    "    'LR_CRITIC': 0.001,                   # learning rate of critic optimizer\n",
    "    'WEIGHT_DECAY': 0.0005,               # weight decay of both optimizers\n",
    "    'BATCH_SIZE': 100,                    # size of mini-batch to fetch from memory store during training\n",
    "    'DISCOUNT': 0.99,                     # discount factor (gamma) to use\n",
    "    'TAU': 0.001,                         # factor to use for soft update of target parameters\n",
    "    'POLICY_NOISE': 0.2,                  # noise added to policy smoothing (sections 5.3, 6.1)\n",
    "    'NOISE_CLIP': 0.5,                    # noise clip for policy smoothing (sections 5.3, 6.1)\n",
    "    'REWARD_SCALE': 1.0,                  # reward scaling factor used\n",
    "    'POLICY_FREQ': 2,                     # update frequence for target networks (sections 5.2, 6.1)\n",
    "    'ACTION_REPEAT': 3,                   # how often the Task should get next_timestamp per step\n",
    "    'BUFFER_SIZE': 1_000_000,             # replay memory buffer size\n",
    "    'EXPLORATION_NOISE': 0.5,             # noise from normal distribution to add during exploration (paper table 3)\n",
    "    'NUM_EPISODES': 2000,                 # total number of episodes\n",
    "    'TASK_RUNTIME': 5.0,                  # the time horizon of a single task\n",
    "}\n",
    "\n",
    "# create global list to hold episodic rewards -> for logging\n",
    "all_episode_reward = list()\n",
    "\n",
    "# create global deque to hold just the last 100 rewards -> for logging\n",
    "last_rewards = deque(maxlen=100)\n",
    "\n",
    "# initialize the task to solve\n",
    "# the quadcopter should forward in every direction\n",
    "task = Task(init_pose=np.array([10., 10., 0., 0., 0., 0.]),\n",
    "            init_velocities=np.array([0.1, 0.1, 0.]),\n",
    "            init_angle_velocities=np.array([0., 0., 0.]),\n",
    "            target_pos=np.array([20., 20., 110., 0., 0., 0.]),\n",
    "            runtime=config.get('TASK_RUNTIME', 5.0),\n",
    "            action_repeat=config.get('ACTION_REPEAT', 3))\n",
    "\n",
    "# initialize the agent\n",
    "agent = TD3Agent(task=task, parameters=config)\n",
    "\n",
    "# PSEUDO CODE: initialize replay buffer\n",
    "memory = ReplayBuffer(config.get('BUFFER_SIZE', 1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start training\n",
    "for i_episode in range(1, config.get('NUM_EPISODES', 1000)+1):\n",
    "    steps = 0\n",
    "    episode_reward = 0.\n",
    "    reward = 0.\n",
    "    state = agent.reset()\n",
    "    while True:\n",
    "        # PSEUDO CODE: select action with exploration noise\n",
    "        # Paper section 6.1 states that a Gaussian noise instead\n",
    "        # of Ornstein-Uhlenbeck was used because the latter offered\n",
    "        # no performance benefits\n",
    "        noise = np.random.normal(0, config.get('EXPLORATION_NOISE', 0.1), \n",
    "                                 size=task.action_size).clip(task.action_low, task.action_high)\n",
    "        action = agent.act(state)\n",
    "        action += noise\n",
    "        \n",
    "        # PSEUDO CODE: execute action in environment\n",
    "        next_state, reward, done = task.step(action)\n",
    "        \n",
    "        # PSEUDO CODE: store transition tuple (state, action, reward, next_state) in memory buffer\n",
    "        # ignore 'done' signal if maximum runtime is reached\n",
    "        done_bool = 0. if task.sim.time + task.sim.dt > task.sim.runtime else float(done)\n",
    "        memory.add((state, action, reward, next_state, done_bool))\n",
    "        \n",
    "        # sum up reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # progress state\n",
    "        state = next_state\n",
    "        \n",
    "        # increment step#\n",
    "        steps += 1\n",
    "        \n",
    "        # PSEUDO CODE: After each time step, the networks are trained with a\n",
    "        # mini-batch of a 100 transitions, sampled uniformly from a replay buffer\n",
    "        # containing the entire history of the agent.\n",
    "        # --- like the implementation on GitHub training is delayed until end\n",
    "        #     of a episode\n",
    "        if done:\n",
    "            # add cumulative reward to global lists\n",
    "            all_episode_reward.append(episode_reward)\n",
    "            last_rewards.append(episode_reward)\n",
    "            \n",
    "            # train using a mini batch as often as steps done in this episode\n",
    "            agent.update(memory=memory, episode_steps=steps)\n",
    "            \n",
    "            # print progress after each episode\n",
    "            print('\\rEpisode {:4d} used {:4d} steps, reward = {:7.3f}, average (100) = {:7.3f}, [{:3d}][{:3d}][{:3d}]'.format(\n",
    "                i_episode, steps, episode_reward, np.mean(last_rewards), int(task.sim.pose[0]), int(task.sim.pose[1]), int(task.sim.pose[2])), end=\"\")\n",
    "            break\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the Rewards\n",
    "\n",
    "Once you are satisfied with your performance, plot the episode rewards, either from a single run, or averaged over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plot the rewards.\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.grid(True)\n",
    "plt.plot(all_episode_reward, '.', alpha=0.5, color='red')\n",
    "plt.plot(np.convolve(all_episode_reward, np.ones(21)/21)[(21-1)//2:-21], color='red', label='Reward per Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.legend(loc=2)\n",
    "plt.xlim(0, len(all_episode_reward))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflections\n",
    "\n",
    "**Question 1**: Describe the task that you specified in `task.py`.  How did you design the reward function?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "I decided to let the Quadcopter start at an initial position and take some units in every direction, the largest in the Z-direction. \n",
    "\n",
    "In the first versions rewards were given in a _positive_ way. The idea was to only punish the agent if the quadcopter hits the wall. That means if the position (either x,y, or z) is at the lower or upper bounds of the environment. The reward was designed as \n",
    "\n",
    "- start with a zero reward\n",
    "- calculate the distance between the points (current position and target position) and add this distance between the points as reward if the new distance is smaller than the initial distance. For example: if the distance between target and initial position is 10 and the distance between target and current position is 2 then the reward is 8. This reward is only given if the distance between the quadcopter and the target position decreased. In addition this reward is multiplied by 100 to get a high reward if the distance decreases.\n",
    "- the only punishment is to subtract 10 points if the wall is touched\n",
    "- for every x, y, or z position that matches the target position a reward of 1 is added\n",
    "- for positive velocity in any direction a reward of 1 is added\n",
    "- if x, y, and z positions of the target position are reached a reward of 1000 is given\n",
    "\n",
    "Additionally I tried to punish the distance by subtract the calculated distance of the reward if the distance increased. Hence the code was\n",
    "\n",
    "        if current_distance < initial_distance:\n",
    "            reward += initial_distance - current_distance\n",
    "        else:\n",
    "            reward -= current_distance\n",
    "\n",
    "But adding the `else` branch did not improve the performance.\n",
    "\n",
    "--- \n",
    "\n",
    "Finally I decided to give no reward for constant running and only add some small rewards (1) whenever one of the positions are reached (x, y, or z). If all positions were reached at the same time I add 10 to the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss your agent briefly, using the following questions as a guide:\n",
    "\n",
    "- What learning algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "As written above I decided to implement `Twin Delayed Deep Deterministic Policy Gradients (TD3)`. This decision was made by just reading a lot about reinforcement learning. I first read \n",
    "\n",
    "- [Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning)\n",
    "- [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)\n",
    "\n",
    "After that I read about the mentioned algorithms. I did not read all papers in detail, but skimmed all of them to find the submitted solution.\n",
    "\n",
    "- [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971v5)\n",
    "- [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n",
    "- [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)\n",
    "- [Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation](https://arxiv.org/abs/1708.05144)\n",
    "- [Continuous Deep Q-Learning with Model-based Acceleration](https://arxiv.org/abs/1603.00748)\n",
    "- [OpenAI Baselines:ACKTR & A2C](https://openai.com/blog/baselines-acktr-a2c/)\n",
    "- [Off-policy evaluation for MDPs with unknown structure](https://arxiv.org/abs/1502.03255)\n",
    "- [Reinforcement Learning (DQN) Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)\n",
    "- [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495)\n",
    "- [Learning Multi-Level Hierarchies with Hindsight](https://arxiv.org/abs/1712.00948)\n",
    "- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
    "- [Distributed Distributional Deterministic Policy Gradients](https://arxiv.org/abs/1804.08617)\n",
    "- [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)\n",
    "- [Learning to reach by reinforcement learning using a receptive field based function approximation approach with continuous actions](https://link.springer.com/article/10.1007/s00422-009-0295-8)\n",
    "\n",
    "As the **Instructions and Hints** in the classroom suggested _DDPG_ I decided to implement TD3 because this tries to address some of the *issues* found in DDPG. And I have to admit that I didn't want to just copy and paste the code given by Udacity.\n",
    "\n",
    "Unfortunately I made several mistakes during the implementation (read below) and I am short of time now. Hence I did not try other algorithms. I wanted to compare TD3 with [SAC](https://arxiv.org/abs/1812.05905) and/or [PPO](https://arxiv.org/abs/1707.06347) but only implemented TD3.\n",
    "\n",
    "The agent is using PyTorch as I read that PyTorch and Caffe2 were joined. And as we only used Keras and Tensorflow I wanted to learn a bit of PyTorch as well.\n",
    "\n",
    "Pseudo code of TD3 is given above. I just followed this for the implementation. The reference implementation used only one critic network logically split into two. That means that the first three layers are used as Q1 and the other three as Q2. I implemented both critics as separate models.\n",
    "\n",
    "The replay buffer used first was a simple deque based one. Initially the neural nets used were using just the same layout as in the research paper of TD3.\n",
    "\n",
    "Actor networks:\n",
    "\n",
    "    (state dim, 400)\n",
    "    ReLU\n",
    "    (400, 300)\n",
    "    ReLU\n",
    "    (300, action dim)\n",
    "    tanh\n",
    "    \n",
    "Critic networks:\n",
    "\n",
    "    (state dim + action dim, 400)\n",
    "    ReLU\n",
    "    (400, 300)\n",
    "    RelU\n",
    "    (300, 1)\n",
    "\n",
    "In the run above I switched the layout as inspired by [CEM-RL](https://arxiv.org/abs/1810.01222) to\n",
    "\n",
    "Actor networks\n",
    "\n",
    "    (state dim, 400)\n",
    "    tanh\n",
    "    (400, 300)\n",
    "    tanh\n",
    "    (300, action dim)\n",
    "    tanh\n",
    "    \n",
    "Critic networks\n",
    "\n",
    "    (state dim + action dim, 400)\n",
    "    Leaky ReLU\n",
    "    (400, 300)\n",
    "    Leaky RelU\n",
    "    (300, 1)\n",
    "\n",
    "Additionally I implemented a ReplayBuffer that uses a softmax-Function over the rewards saved in the buffer. That way it samples the transitions having the highest rewards first. \n",
    "\n",
    "The hyper parameters used first are the same parameters that were used in the research paper as well. \n",
    "\n",
    "| Parameter | value   |\n",
    "|-----------|---------|\n",
    "| $\\gamma$ | 0.99 |\n",
    "| $\\tau$ | 0.05 |\n",
    "| $\\epsilon$ | $\\mathcal{N}$ (0, 0.2) clipped to (-0.5, 0.5) |\n",
    "| $\\alpha$ of actor optimizer  | 0.01 |\n",
    "| $\\alpha$ of critic optimizer | 0.01 |\n",
    "| Optimizer of all networks | Adam |\n",
    "| Reward scaling | 1.0 |\n",
    "| Exploration noise | $\\mathcal{N}$ (0, 0.1) |\n",
    "| Mini-Batch size | 100 |\n",
    "\n",
    "As written in the research paper I used a update frequency of **2**, hence the target networks are only updated every second learning cycle.\n",
    "\n",
    "Additionally I decided to manually initialize the network weights randomly drawn from a uniform distribution from (-0.005, 0.005) without weight decay for the optimizer.\n",
    "\n",
    "During training the agent had some problems (read below) and the hyper parameters finally used can be seen in the config dict above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "As seen in the reward plot and the final position of the quadcopter the task was not learned by this agent. I think that I choosed one of the hardest tasks: Starting at one point and blindly find another one. I assume that lifting or landing the quadcopter is easier to train. \n",
    "\n",
    "I assume that I did not find a reward function good enough to help the agent. The other things I tried: \n",
    "\n",
    "- running for more number of periods (2000 instead of 1000)\n",
    "- changed the network architecture\n",
    "- changed learning rates of actor and critics\n",
    "- changed discount\n",
    "- changed tau\n",
    "- changed exploration noise\n",
    "- changed replay buffer implementation\n",
    "- changed the task runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, plotting, specifying the task, etc.)\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "This project was very hard for me. I realized that I did not fully understand the concepts during the classroom sessions and the mini projects. I finally get the concepts after reading a lot of papers (not only the list above) and blog posts (mainly on medium.com). \n",
    "\n",
    "Then I tried to start by implementing some of the replay buffer concepts first as this class is used as utility class and I wanted to have that done first. I tried the simple replay buffer (easy to understand), the rank based prioritized replay buffer (I used a heapq), the td error based prioritized replay buffer (using a binary segment tree) and implemented two by myself that used the gained rewards as priorization. The first one works as:\n",
    "\n",
    "    split the length of the deque into sample_size / 2 segments\n",
    "    for every segment\n",
    "        get the index of the sample that contains the highest reward\n",
    "        add this sample to the return list\n",
    "        if this index is at the first position of the segment\n",
    "            add the next sample to the return list\n",
    "        else\n",
    "            add the previous sample to the return list\n",
    "    return sample list\n",
    "\n",
    "The second one just stores the observations in two deque objects: One holding all observations and the second holding only the rewards gained. During sampling I use `softmax` to squeeze the rewards into the probability range 0-1 and use that with `numpy.random.choice` to sample the observations having the highest rewards (what means that training using the sample are biased because most of the time the same observations will be sampled over and over again.)\n",
    "\n",
    "After that I needed to learn how to use PyTorch. Thanks to the excellent documentation this was easy enough. I implemented the TD3 agent and the Actor and Critic models as described above. But something went wrong. I made two errors:\n",
    "\n",
    "1. the usage of the replay buffer was not implemented correct\n",
    "2. I did some errors implementing and training the Critic networks\n",
    "\n",
    "Fixing that took too much time. Hence I just come back to use the softmax replay buffer (that uses the same structure as the simple replay buffer) and finally got the Critic correct (at least I think so). \n",
    "\n",
    "One very bad problem remaines: Whatever I do my agent stucks. As written above I changed the reward function several times. I changed the hyper parameters several times. I change the network architecture. But the agent hucks the quadcopter to the area boundary very soon somtimes and then learns to stay with that. For example in several runs the Z-Position was trained to 300. Or the Y-Position remained at 0 all the time. I change the Z-position for inital and target position and set the initial velocity in the Z-Direction to 0. That just freezes the Z-position to around 127 (instead of 110). X and Y positions are not learned at all. \n",
    "\n",
    "I assume that I'm not able to find a reward function that helps the agent to understand the task at hand. But I do not want to train a startup-task or a hover task. It should be possible to train the agent to find the target position.\n",
    "\n",
    "In addition I did not understand `physics_sim.py`. Perhaps I need to change state or action size. But I have no idea about how to do that. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
